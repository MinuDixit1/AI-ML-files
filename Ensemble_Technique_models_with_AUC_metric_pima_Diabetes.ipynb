{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context :    \n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "Columns details are as below : \n",
    "- Pregnancies : Number of times pregnant\n",
    "- Glucose : Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- BloodPressure : Diastolic blood pressure (mm Hg)\n",
    "- SkinThickness : Triceps skin fold thickness (mm)\n",
    "- Insulin : 2-Hour serum insulin (mu U/ml)\n",
    "- BMI : Body mass index (weight in kg/(height in m)^2)\n",
    "- DiabetesPedigreeFunction : Diabetes pedigree function\n",
    "- Age : Age (years)\n",
    "- Outcome : Class variable (0 or 1) 268 of 768 are 1, the others are 0\n",
    "\n",
    "So, here the y-variable is Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries : \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['preg', 'glu', 'bp', 'sft', 'ins', 'bmi', 'dpf', 'age', 'outcome']\n",
    "prima_df = pd.read_csv(\"prima-indians-diabetes.data\",names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=prima_df[['preg', 'glu', 'bp', 'sft', 'ins', 'bmi', 'dpf', 'age']]\n",
    "Y=prima_df['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing x-variables : \n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "base_knn=KNeighborsClassifier(n_neighbors=7,weights='distance')\n",
    "base_nb=GaussianNB()\n",
    "base_LR=LogisticRegression(random_state=2)\n",
    "base_rf=RandomForestClassifier(n_estimators=101,random_state=2)\n",
    "gb_model=GradientBoostingClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_knn=BaggingClassifier(base_estimator=base_knn,n_estimators=17,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_LR=BaggingClassifier(base_estimator=base_LR,n_estimators=15,random_state=2)\n",
    "boost_LR = AdaBoostClassifier(base_estimator=base_LR,n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_nb=BaggingClassifier(base_estimator=base_nb,n_estimators=15,random_state=2)\n",
    "boost_nb = AdaBoostClassifier(base_estimator=base_nb,n_estimators=51,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_rf=AdaBoostClassifier(base_estimator=base_rf,n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_dt=BaggingClassifier(n_estimators=15,random_state=2)\n",
    "boost_dt = AdaBoostClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = VotingClassifier(estimators = [('Boosted_LR',boost_LR),('RF', base_rf), ('Boosted_DT', boost_dt)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are performing Ensemble Techniques. Below models are being built :\n",
    "- Boosted Linear Regression - n_estimators=50\n",
    "- Random Forest - n_estimators=50\n",
    "- Boosted Decsion Tree - n_estimators=50\n",
    "- Gradient Boosting - n_estimators=50\n",
    "- Stacked Model - This is not a single model but 3 models are stacked together as one : Boosted Linear Regression, Random Forest and Boosted Decsion Tree (with stacking we can use multiple models in one model and check the performance).\n",
    "\n",
    "Now, all the above models are provided data in 5-folds - Using k-fold cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted for Healthy: 0.83 (+/- 0.00048) [BoostLR]\n",
      "f1_weighted for Diabetic: 0.62 (+/- 0.00222) [BoostLR]\n",
      "f1_weighted for Healthy: 0.81 (+/- 0.00029) [RF]\n",
      "f1_weighted for Diabetic: 0.62 (+/- 0.00171) [RF]\n",
      "f1_weighted for Healthy: 0.81 (+/- 0.00028) [BoostedDT]\n",
      "f1_weighted for Diabetic: 0.60 (+/- 0.00683) [BoostedDT]\n",
      "f1_weighted for Healthy: 0.81 (+/- 0.00046) [GradientBoost]\n",
      "f1_weighted for Diabetic: 0.61 (+/- 0.00254) [GradientBoost]\n",
      "f1_weighted for Healthy: 0.81 (+/- 0.00020) [stacked]\n",
      "f1_weighted for Diabetic: 0.62 (+/- 0.00166) [stacked]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "for model, name in zip([boost_LR,base_rf,boost_dt,gb_model,stacked], ['BoostLR','RF','BoostedDT','GradientBoost','stacked']):\n",
    "    k=0\n",
    "    recall=np.zeros((2,5))\n",
    "    prec=np.zeros((2,5))\n",
    "    fscore=np.zeros((2,5))\n",
    "    for train,test in kf.split(X,Y):\n",
    "        Xtrain,Xtest=X[train,:],X[test,:]\n",
    "        Ytrain,Ytest=Y[train],Y[test]\n",
    "        model.fit(Xtrain,Ytrain)\n",
    "        Y_predict=model.predict(Xtest)\n",
    "        cm=metrics.confusion_matrix(Ytest,Y_predict)\n",
    "        for i in np.arange(0,2):\n",
    "            recall[i,k]=cm[i,i]/cm[i,:].sum()\n",
    "        for i in np.arange(0,2):\n",
    "            prec[i,k]=cm[i,i]/cm[:,i].sum()\n",
    "        k=k+1\n",
    "    for row in np.arange(0,2):\n",
    "        for col in np.arange(0,5):\n",
    "            fscore[row,col]=2*(recall[row,col]*prec[row,col])/(recall[row,col]+prec[row,col])\n",
    "    print(\"f1_weighted for Healthy: %0.02f (+/- %0.5f) [%s]\" % (np.mean(fscore[0,:]), np.var(fscore[0,:],ddof=1), name ))   \n",
    "    print(\"f1_weighted for Diabetic: %0.02f (+/- %0.5f) [%s]\" % (np.mean(fscore[1,:]), np.var(fscore[1,:],ddof=1), name ))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC scores: 0.72 (+/- 0.00086) [BoostLR]\n",
      "AUC scores: 0.71 (+/- 0.00047) [RF]\n",
      "AUC scores: 0.70 (+/- 0.00229) [BoostedDT]\n",
      "AUC scores: 0.71 (+/- 0.00100) [GradientBoost]\n",
      "AUC scores: 0.71 (+/- 0.00042) [stacked]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "kf=KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "for model, name in zip([boost_LR,base_rf,boost_dt,gb_model,stacked], ['BoostLR','RF','BoostedDT','GradientBoost','stacked']):\n",
    "    roc_auc=[]\n",
    "    for train,test in kf.split(X,Y):\n",
    "        Xtrain,Xtest=X[train,:],X[test,:]\n",
    "        Ytrain,Ytest=Y[train],Y[test]\n",
    "        model.fit(Xtrain,Ytrain)\n",
    "        Y_predict=model.predict(Xtest)\n",
    "        cm=metrics.confusion_matrix(Ytest,Y_predict)\n",
    "        fpr,tpr, _ = roc_curve(Ytest,Y_predict)\n",
    "        roc_auc.append(auc(fpr, tpr))\n",
    "    print(\"AUC scores: %0.02f (+/- %0.5f) [%s]\" % (np.mean(roc_auc), np.var(roc_auc,ddof=1), name ))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Clearly Boosted Linear Regresson Forest model is outperforming other models, F1 score and AUC scores are displayed above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
